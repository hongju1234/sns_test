
from bs4 import BeautifulSoup
from selenium import webdriver
import time
import sys
import re
import math
import numpy  
import pandas as pd  
import xlwt

import random
import os  

from selenium.webdriver.support.ui import Select

#Step 2. 사용자에게 검색어 키워드를 입력 받습니다.
print("=" *80)
print("블로그 크롤러 : 여러건의 네이버 블로그 정보 추출하여 저장하기")
print("=" *80)

query_txt = input('크롤링할 키워드는 무엇입니까?: ')
include_w = input('결과에서 반드시 포함하는 단어를 입력하세요: ')
exclude_w = input('결과에서 제외할 단어를 입력하세요: ')
start_date = input('조회 시작일자 입력: ')
end_date = input('조회 종료일자 입력: ')
cnt = int(input('크롤링 할 건수는 몇건입니까?: '))
# 학습목표 1: 현재 크롤링 시점의 날짜로 폴더 이름을 자동으로 생성하기
f_dir = input("결과 파일을 저장할 폴더명만 쓰세요(예:c:\\temp\\):")


# 저장될 파일위치와 이름을 지정합니다
n = time.localtime()
s = '%04d-%02d-%02d-%02d-%02d-%02d' % (n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)

os.makedirs(f_dir+s+'-'+query_txt)
os.chdir(f_dir+s+'-'+query_txt)

ff_name=f_dir+s+'-'+query_txt+'\\'+s+'-'+query_txt+'.txt'
#fc_name=f_dir+s+'-'+query_txt+'\\'+s+'-'+query_txt+'.csv'
fx_name=f_dir+s+'-'+query_txt+'\\'+s+'-'+query_txt+'.xls'


#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.

s_time = time.time( )

path = "c:/temp/chromedriver_240/chromedriver.exe"
driver = webdriver.Chrome(path)

driver.get('https://section.blog.naver.com/BlogHome.naver?directoryNo=0&currentPage=1&groupId=0')

time.sleep(random.randrange(2,5))  # 2 - 5 초 사이에 랜덤으로 시간 선택

# 학습목표 2: 드롭박스로 보이는 메뉴를 선택하는 방법을 배운다.
#Step 4. 모든 여행지 링크를 선택합니다

element = driver.find_element(By.NAME, 'sectionBlogQuery')
element.send_keys(query_txt)
driver.find_element(By.CLASS_NAME, "button button_blog").click()
time.sleep(2)


# Step 6: 사용자 요청 건수가 실제 검색 건수보다 많을 경우
# 실제 검색 건수로 리셋하기

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')

r_cnt= soup.find('div', class_='total_check').find('span').get_text( )
r_cnt2 = r_cnt.replace(",","")
search_cnt = int(r_cnt2)

if cnt > search_cnt :
    cnt = search_cnt

print("전체 검색 결과 건수 :",search_cnt,"건")
print("실제 최종 출력 건수",cnt)

print("\n")
page_cnt = math.ceil(cnt / 10)
print("크롤링 할 총 페이지 번호: ",page_cnt)
print("=" *80)

# Step 7. 페이지를 변경하면서 사용자가 요청한 건수만큼 내용을 추출하여 파일에 저장하기

no2=[]           # 게시글 번호 컬럼
contents=[ ]     # 게시글 내용 컬럼
no = 1

for x in range(1,page_cnt+1) :
    print("%s 페이지 내용 수집 시작합니다 =======================" %x)
    
    for i in range(1,11):  
        f = open(ff_name, 'a',encoding='UTF-8')
        
        if no > cnt :
            break
            
        #각 게시글의 제목 누르기    
        driver.find_element_by_xpath("""//*[@id="contents"]/div[2]/div[1]/ul/li[%s]/div[2]/div/a""" % i).click( )
        time.sleep(2)
        
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')        
        content_list = soup.find('div','wrap_contView')    
        #content_top = content_list.find('div','area_txtView top').find('div','inr_wrap').find('div','inr')
        con_1 = content_list.find('p').get_text()
        print(no , ':  ', con_1)
        
        f.write(str(no) + ': '+ str(con_1) + "\n")
        f.close( )
        
        print("\n")
        no2.append(no)
        contents.append(con_1)
    
        driver.back( )  # 뒤로 돌아가기 기능
        time.sleep(2)
                
        no += 1
        
    if x > page_cnt+1 :
            break
        
    x += 1
    
    if (x % 5 == 0):
            driver.find_element_by_link_text('''다음''').click()
    else :
            driver.find_element_by_link_text("""%s""" %x).click() # 다음 페이지번호 클릭    
    time.sleep(2)

# Step 8. 출력 결과를 표(데이터 프레임) 형태로 만들어 csv,xls 형식으로 저장하기
korea = pd.DataFrame()
korea['번호']=no2
korea['내용']=contents
        
# 엑셀 형태로 저장하기
korea.to_excel(fx_name,index=False)

e_time = time.time( )     # 검색이 종료된 시점의 timestamp 를 지정합니다
t_time = e_time - s_time

# Step 9. 요약정보 보여주기
print("\n") 
print("=" *80)
print("총 소요시간은 %s 초 입니다 " %round(t_time,1))
print("파일 저장 완료: txt 파일명 : %s " %ff_name)
print("파일 저장 완료: xls 파일명 : %s " %fx_name)
print("=" *80)

driver.close( )
